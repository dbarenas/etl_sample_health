# ETL Pipeline for Patient and Device Data

## 1. Overview

This project implements an ETL (Extract, Transform, Load) pipeline designed to process patient demographic data and medical device readings. It extracts data from JSON and CSV files, transforms it according to defined schemas and validation rules, and then loads the processed data and any identified errors into a PostgreSQL database. The pipeline is built with Python 3.11+, uses Pydantic for data validation, and can be containerized using Docker.

## 2. Features Implemented

*   **Extraction**:
    *   Reads patient data from JSON files.
    *   Reads device reading data from CSV files.
    *   Handles file not found and malformed file errors.
    *   Unified `extract_data` function to orchestrate data ingestion.
*   **Transformation**:
    *   **Schema Validation**: Uses Pydantic models (`etl/schemas.py`) to define and enforce schemas for patient and device data.
        *   `Patient` schema: `id`, `name`, `dob`, `gender`, `address`, `email`, `phone`, `sex`.
        *   `DeviceReading` schema: `id` (formerly `reading_id`), `patient_id`, `timestamp`, `glucose`, `systolic_bp`, `diastolic_bp`, `weight`.
    *   **Data Normalization**:
        *   Converts date of birth (DOB) to `YYYY-MM-DD` format (accepts `MM/DD/YYYY` as input).
        *   Normalizes `gender` and `sex` fields to title case (e.g., "Female").
    *   **Validation Rules**:
        *   Validates email and phone number formats (basic regex).
        *   Ensures numeric fields (glucose, BP, weight) are within plausible ranges.
        *   Checks timestamp format (ISO 8601).
    *   **Error Handling**:
        *   Identifies records failing validation (missing fields, invalid formats, outliers, etc.).
        *   Logs logical inconsistencies (e.g., diastolic BP > systolic BP).
        *   Checks for timestamp order for device readings (per patient if `patient_id` is available, otherwise globally).
        *   Generates structured `ErrorRecord` objects for each issue, detailing the problem.
*   **Loading**:
    *   Loads successfully transformed `Patient` and `DeviceReading` objects into respective PostgreSQL tables.
    *   Loads `ErrorRecord` objects into a PostgreSQL table for errors identified during transformation.
    *   Handles potential database errors during loading (e.g., duplicate primary keys via `ON CONFLICT DO NOTHING`).
    *   Initializes database schema (creates tables if they don't exist) upon pipeline startup.
*   **Asynchronous Pipeline**:
    *   The main pipeline (`main.py`) orchestrates extraction, transformation, and loading steps asynchronously using `asyncio` and `run_in_executor` for potentially blocking operations.
*   **Database Utilities**:
    *   `etl/db_utils.py` provides helper functions for database connection and DDL execution.
*   **Configuration**:
    *   File paths for sample data are defined in `main.py`.
    *   Database connection parameters are primarily sourced from environment variables (defaults provided in `etl/db_utils.py` and `docker-compose.yml`).
    *   Pydantic models in `etl/schemas.py` centralize data validation rules.
*   **Dockerization**:
    *   `Dockerfile` provided to build a container image for the application, including system dependencies for `psycopg2`.
    *   `docker-compose.yml` for easy multi-container setup (ETL app + PostgreSQL database).
*   **Unit Tests**:
    *   Comprehensive unit tests for extraction, transformation, and (in-memory parts of) loading modules (`tests/`). *Note: Database interaction parts of loading are currently tested via the `if __name__ == '__main__'` block in `etl/loading.py` and by running the full pipeline.*

## 3. Code Structure

```
.
├── Dockerfile              # For building the Docker container
├── docker-compose.yml      # For Docker Compose setup (app & database)
├── main.py                 # Main script to run the ETL pipeline
├── requirements.txt        # Python dependencies (pydantic, psycopg2-binary)
├── README.md               # This file
├── data/                   # Directory for input data files (created by main.py if not present)
│   ├── patients.json       # Sample patient data (generated by main.py)
│   └── device_readings.csv # Sample device readings data (generated by main.py)
├── etl/                    # Core ETL logic package
│   ├── __init__.py
│   ├── db_utils.py         # Database connection and DDL utilities
│   ├── extraction.py       # Data extraction functions
│   ├── schemas.py          # Pydantic schema definitions
│   ├── transformation.py   # Data transformation and validation functions
│   └── loading.py          # Data loading functions (to PostgreSQL)
└── tests/                  # Unit tests
    ├── __init__.py
    ├── test_extraction.py
    ├── test_transformation.py
    └── test_loading.py     # Tests for older in-memory loading logic
```

## 4. Database Schema

The ETL pipeline loads data into the following PostgreSQL tables. DDL for these tables is executed automatically by the pipeline if they do not exist.

### `patients` Table
```sql
CREATE TABLE IF NOT EXISTS patients (
    id VARCHAR(255) PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    dob DATE,
    gender VARCHAR(50),
    address TEXT,
    email VARCHAR(255),
    phone VARCHAR(50),
    sex VARCHAR(50)
);
```

### `device_readings` Table
```sql
CREATE TABLE IF NOT EXISTS device_readings (
    id VARCHAR(255) PRIMARY KEY,
    patient_id VARCHAR(255) REFERENCES patients(id) ON DELETE CASCADE,
    timestamp TIMESTAMPTZ NOT NULL,
    glucose NUMERIC(8, 2),
    systolic_bp INTEGER,
    diastolic_bp INTEGER,
    weight NUMERIC(8, 2)
);

CREATE INDEX IF NOT EXISTS idx_device_readings_patient_id_timestamp ON device_readings(patient_id, timestamp);
```

### `error_records` Table
```sql
CREATE TABLE IF NOT EXISTS error_records (
    error_id SERIAL PRIMARY KEY,
    reference TEXT,
    source_table VARCHAR(100),
    field_name VARCHAR(100),
    error_type VARCHAR(100),
    case_description TEXT,
    original_value TEXT,
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
);
```

## 5. Setup Instructions

### Prerequisites
*   Python 3.11+
*   pip (Python package installer)
*   Docker and Docker Compose (if using containerized approach)

### Installation
1.  Clone the repository (or ensure you have the code).
2.  Navigate to the project root directory.
3.  Install dependencies (primarily for local Python execution without Docker, as Docker handles its own dependencies):
    ```bash
    pip install -r requirements.txt
    ```

## 6. How to Run the Pipeline

### Directly with Python
*(Note: This requires a PostgreSQL instance to be running and accessible, with connection details matching environment variables or defaults in `etl/db_utils.py`. For simplicity, using Docker Compose is recommended.)*

1.  Ensure you have completed the setup instructions.
2.  Set environment variables for your database if they differ from defaults (e.g., `DB_HOST`, `DB_PORT`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`).
3.  Run the main script from the project root directory:
    ```bash
    python main.py
    ```
4.  The script will:
    *   Attempt to connect to the PostgreSQL database and initialize the schema.
    *   Create sample `data/patients.json` and `data/device_readings.csv` files if they don't already exist.
    *   Execute the ETL pipeline (Extract, Transform, Load to DB).
    *   Print logs to the console, including summaries of extracted, processed, and error records, and total execution time.

### Using Docker (Standalone Container - Not Recommended for DB Interaction)

Running the app container standalone without Docker Compose is possible but not ideal if it needs to connect to a database (which would typically be another container or an external service). The following commands are for running the app in isolation (e.g., for testing non-DB parts if the app was designed for it). For this project with DB integration, use Docker Compose.

1.  **Build the Docker image**:
    ```bash
    docker build -t etl-pipeline-app .
    ```
2.  **Run the Docker container**:
    This would require setting up environment variables for DB connection if the pipeline is to run successfully.
    ```bash
    # Example - this would fail if DB_HOST 'db' is not resolvable
    # docker run --rm -e DB_HOST=localhost -e ... etl-pipeline-app 
    ```

### Using Docker Compose (Recommended for Local Development)

This project includes a `docker-compose.yml` file that sets up both the ETL application container and a PostgreSQL database container. This is the recommended way to run the application locally.

1.  **Ensure Docker Compose is installed.** (It's usually included with Docker Desktop).
2.  **Build and start the services**:
    Navigate to the project root directory and run:
    ```bash
    docker-compose up --build
    ```
    The `--build` flag ensures the image is rebuilt if there are changes to the `Dockerfile` or application code.
    This command will start both the PostgreSQL database and the ETL application. The ETL application (`app` service) will run `python main.py`.

3.  **To stop the services**:
    Press `Ctrl+C` in the terminal where `docker-compose up` is running, then run:
    ```bash
    docker-compose down
    ```
    This will stop and remove the containers. The PostgreSQL data will persist in a Docker volume (`pgdata`) unless the volume is explicitly removed.

**Note on Database Integration**:
This Docker Compose setup includes a PostgreSQL database, and the Python ETL scripts (`etl/loading.py`) now load transformed data directly into this database. The `psycopg2-binary` library is used for this interaction. Environment variables for database connection are configured in `docker-compose.yml` for the `app` service and used by the Python application.

## 7. Example of Pipeline Execution

When you run `python main.py` (preferably via `docker-compose up`), you will see output similar to the following (counts and specific messages may vary based on sample data and run conditions):

```
Running ETL Pipeline with PostgreSQL Integration...
Attempting to initialize database schema...
Successfully connected to PostgreSQL database.
DDL statements executed successfully.
Database schema initialized (or already exists).
Database connection closed.
Successfully connected to PostgreSQL database.
Created sample data: data/patients.json
Created sample data: data/device_readings.csv
Starting data extraction...
Extraction completed in X.XX seconds. Patients: 5, Devices: 7
Starting data transformation...
Transformation completed in Y.YY seconds. Valid Patients: A, Valid Readings: B, Transformation error records: C
Starting data loading into database...
Database loading completed in Z.ZZ seconds.

--- ETL Pipeline Summary ---
Total execution time: T.TT seconds
Patients extracted: 5
Device readings extracted: 7
Valid patients for DB: A
Valid device readings for DB: B
Transformation error records: C

--- Database Loading Summary ---
Successfully loaded patients to DB: A'
Successfully loaded device readings to DB: B'
Database Loading Errors (Data): [...] (List of errors if any)
Successfully loaded transformation error records to DB: C'
Database Loading Errors (Error Records): [...] (List of errors if any)
Database connection closed.
ETL Pipeline finished.
```
*(A, B, C, A', B', C', X, Y, Z, T are placeholders for actual numbers from an execution run. A' might differ from A if some valid patients fail DB insertion due to constraints not caught in transformation, e.g. unexpected duplicate IDs if ON CONFLICT wasn't used or if IDs aren't truly unique in source.)*

### Verifying Data in PostgreSQL

After the pipeline runs (ideally via `docker-compose up`), you can connect to the PostgreSQL database to verify that data has been loaded.

1.  **Connect to the PostgreSQL container**:
    Open a new terminal and run:
    ```bash
    docker-compose exec db psql -U etl_user -d etl_data
    ```
    You will be prompted for the password, which is `etl_password` (as defined in `docker-compose.yml`).

2.  **Example SQL Queries**:
    Once connected via `psql`, you can run queries like:
    ```sql
    -- Count records in each table
    SELECT COUNT(*) FROM patients;
    SELECT COUNT(*) FROM device_readings;
    SELECT COUNT(*) FROM error_records;

    -- View some sample data
    SELECT * FROM patients LIMIT 5;
    SELECT * FROM device_readings LIMIT 5;
    SELECT * FROM error_records LIMIT 5;

    -- Find readings for a specific patient
    SELECT dr.*
    FROM device_readings dr
    JOIN patients p ON dr.patient_id = p.id
    WHERE p.name = 'Alice Wonderland'; -- Or use an actual patient ID like 'p1'
    ```

## 8. Running Unit Tests

To run the unit tests, navigate to the project root directory and execute:

```bash
python -m unittest discover -s tests -v
```

This command will discover and run all tests within the `tests` directory and provide verbose output. You should see all tests passing. The tests primarily cover extraction, transformation, and the older in-memory loading logic. Testing database interactions currently relies on the `if __name__ == '__main__'` block in `etl/loading.py` or by running the full pipeline.
```
