# ETL Pipeline for Patient and Device Data

## 1. Overview

This project implements an ETL (Extract, Transform, Load) pipeline designed to process patient demographic data and medical device readings. It extracts data from JSON and CSV files, transforms it according to defined schemas and validation rules, and then loads the processed data and any identified errors into in-memory structures. The pipeline is built with Python 3.11+, uses FastAPI for potential future API exposure (though not currently implemented as an API), Pydantic for data validation, and can be containerized using Docker.

## 2. Features Implemented

*   **Extraction**:
    *   Reads patient data from JSON files.
    *   Reads device reading data from CSV files.
    *   Handles file not found and malformed file errors.
    *   Unified `extract_data` function to orchestrate data ingestion.
*   **Transformation**:
    *   **Schema Validation**: Uses Pydantic models (`etl/schemas.py`) to define and enforce schemas for patient and device data.
        *   `Patient` schema: `id`, `name`, `dob`, `gender`, `address`, `email`, `phone`, `sex`.
        *   `DeviceReading` schema: `reading_id`, `patient_id`, `timestamp`, `glucose`, `systolic_bp`, `diastolic_bp`, `weight`.
    *   **Data Normalization**:
        *   Converts date of birth (DOB) to `YYYY-MM-DD` format (accepts `MM/DD/YYYY` as input).
        *   Normalizes `gender` and `sex` fields to title case (e.g., "Female").
    *   **Validation Rules**:
        *   Validates email and phone number formats (basic regex).
        *   Ensures numeric fields (glucose, BP, weight) are within plausible ranges.
        *   Checks timestamp format (ISO 8601).
    *   **Error Handling**:
        *   Identifies records failing validation (missing fields, invalid formats, outliers, etc.).
        *   Logs logical inconsistencies (e.g., diastolic BP > systolic BP).
        *   Checks for timestamp order for device readings (per patient if `patient_id` is available, otherwise globally).
        *   Generates structured `ErrorRecord` objects for each issue, detailing the problem.
*   **Loading**:
    *   Loads successfully transformed `Patient` and `DeviceReading` objects into in-memory lists.
    *   Loads `ErrorRecord` objects into a separate in-memory "quarantine" list.
*   **Asynchronous Pipeline**:
    *   The main pipeline (`main.py`) orchestrates extraction, transformation, and loading steps asynchronously using `asyncio` and `run_in_executor` for potentially blocking operations.
*   **Configuration**:
    *   File paths are defined in `main.py` (can be externalized).
    *   Pydantic models in `etl/schemas.py` centralize data validation rules.
*   **Dockerization**:
    *   `Dockerfile` provided to build a container image for the application.
*   **Unit Tests**:
    *   Comprehensive unit tests for extraction, transformation, and loading modules (`tests/`).

## 3. Code Structure

```
.
├── Dockerfile              # For building the Docker container
├── main.py                 # Main script to run the ETL pipeline
├── requirements.txt        # Python dependencies (pydantic)
├── README.md               # This file
├── data/                   # Directory for input data files (created by main.py if not present)
│   ├── patients.json       # Sample patient data (generated by main.py)
│   └── device_readings.csv # Sample device readings data (generated by main.py)
├── etl/                    # Core ETL logic package
│   ├── __init__.py
│   ├── extraction.py       # Data extraction functions
│   ├── schemas.py          # Pydantic schema definitions
│   ├── transformation.py   # Data transformation and validation functions
│   └── loading.py          # Data loading functions
└── tests/                  # Unit tests
    ├── __init__.py
    ├── test_extraction.py
    ├── test_transformation.py
    └── test_loading.py
```

## 4. Setup Instructions

### Prerequisites
*   Python 3.11+
*   pip (Python package installer)

### Installation
1.  Clone the repository (or ensure you have the code).
2.  Navigate to the project root directory.
3.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

## 5. How to Run the Pipeline

### Directly with Python
1.  Ensure you have completed the setup instructions.
2.  Run the main script from the project root directory:
    ```bash
    python main.py
    ```
3.  The script will:
    *   Create sample `data/patients.json` and `data/device_readings.csv` files if they don't already exist.
    *   Execute the ETL pipeline.
    *   Print logs to the console, including summaries of extracted, processed, and error records, and total execution time.

### Using Docker

1.  **Build the Docker image**:
    Navigate to the project root directory (where the `Dockerfile` is located) and run:
    ```bash
    docker build -t etl-pipeline-app .
    ```
2.  **Run the Docker container**:
    ```bash
    docker run --rm etl-pipeline-app
    ```
    The `--rm` flag automatically removes the container when it exits.
    The container will execute `python main.py` and you will see the same console output as running directly with Python.

## 6. Example of Pipeline Execution

When you run `python main.py` or the Docker container, you will see output similar to the following (counts and specific messages may vary based on sample data):

```
Running ETL Pipeline...
Clearing previous data from in-memory storage...
Created sample data: data/patients.json
Created sample data: data/device_readings.csv
Starting data extraction...
Extraction completed in X.XX seconds. Patients: 5, Devices: 7
Starting data transformation...
Transformation completed in Y.YY seconds. Valid Patients: A, Valid Readings: B, Errors: C
Starting data loading...
Loading completed in Z.ZZ seconds.

--- ETL Pipeline Summary ---
Total execution time: T.TT seconds
Patients extracted: 5
Device readings extracted: 7
Valid patients processed: A
Valid device readings processed: B
Error records generated: C

--- Loading Summary ---
Successfully loaded patients: A
Successfully loaded device readings: B
Successfully loaded error records: C
ETL Pipeline finished.
```
*(Replace A, B, C, X, Y, Z, T with actual numbers from an execution run)*

## 7. Running Unit Tests

To run the unit tests, navigate to the project root directory and execute:

```bash
python -m unittest discover -s tests -v
```

This command will discover and run all tests within the `tests` directory and provide verbose output. You should see all tests passing.
```
